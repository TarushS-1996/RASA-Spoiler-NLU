{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RASA Model Tester\n",
    "\n",
    "In this example, we will load our trained model and pass it messages for testing whether it will detect the intent of spoiler or non spoiler content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import rasa.nlu\n",
    "import rasa.core\n",
    "import spacy\n",
    "\n",
    "from rasa.nlu.training_data import load_data\n",
    "from rasa.nlu.config import RasaNLUModelConfig\n",
    "from rasa.nlu.model import Interpreter \n",
    "from rasa.nlu import config \n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Just like earlier, we have loaded the necessary libraries. In this the difference in imported libraries from RASA is just the Interpreter from the NLU models. This interpreter will load the traiend model and will allow for you to pass custom messages to test out the network. \n",
    "\n",
    "The library termcolor is just for highlighting strings and make them easy to understand and make them presentable. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpreter = Interpreter.load(\"./models/nlu/spoiler_detection\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "We load the model by specifying path to the file in which the model was saved aka the path we used in training the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tinp = input(\"Enter query here ::\")\n",
    "\tinterpret = interpreter.parse(inp)\n",
    "    print(interpret)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now we make a simple input which will take the raw string from the user and pass it to the model via the interpreter me specified in the above line. When we print the \"interpret\" we get a dictionary which will give us things about the statement namely the intent, the confidence, the entity, the type of entity, the confidence of the entity and the location of the entity in ther statement. Basically a lot of information. \n",
    "\n",
    "So lets get the stuff which we are mainly concerned about i.e. intent and entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    intent = interpret['intent']['name']\n",
    "\tentities = interpret['entities']\n",
    "    \"\"\"As entities can be more than one, we store them in alist. However,\n",
    "    storing the entities as a dictionary will make things better when\n",
    "    performing entities based if else conditional statements. \"\"\"\n",
    "\tentity = []\n",
    "\tfor i in range(len(entities)):\n",
    "\t\t#entity.append([entities[i]['value'], entities[i]['start'], entities[i]['end']])\n",
    "\t\tentity.append(entities[i]['value'])\n",
    "        \n",
    "    if intent == 'non-spoiler':\n",
    "\t\tprint(\"Interpret the intent to be :: {}. Detected terms are :: {}\".format(colored(intent, 'green'), colored(entity, 'green')))\n",
    "\tif intent == 'spoiler':\n",
    "\t\tprint(\"Interpret the intent to be :: {}. Detected terms are :: {}\".format(colored(intent, 'red'), colored(entity, 'red')))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Finally we use the library termcolor to make the output presentable. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
